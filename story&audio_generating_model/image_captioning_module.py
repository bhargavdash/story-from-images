# -*- coding: utf-8 -*-
"""image_captioning_module.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z9NzhK8JupVyHCkvS1ysfbCe6VkeYtUu
"""

# We use salesforce blip model for image captioning
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image

# load the model and processor
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# function which accepts a list of image urls and returns a list of captions for each of them
def generate_captions(image_paths):
  # list to store the images to feed the model
  images = []
  # iterate through each path and store the image in the list
  for path in image_paths:
    image = Image.open(path).convert("RGB")
    images.append(image)
  # list to store captions
  captions = []
  # iterate over each image, feed it to the model and generate caption for it
  for image in images:
    inputs = processor(images=image, return_tensors="pt")
    out = model.generate(**inputs)
    caption = processor.decode(out[0], skip_special_tokens=True)
    captions.append(caption)
  # return the captions list
  return captions

# image_paths = ["test-image-1.jpg", "test-image-2.jpg"]
# captions = generate_captions(image_paths)
# for caption in captions:
#   print(caption)